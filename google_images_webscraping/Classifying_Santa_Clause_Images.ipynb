{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports for the Gabor filter\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.filters import gabor_kernel\n",
    "from scipy.stats import kurtosis, skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "1. load Santa and Nosanta images\n",
    "2. use Gabor filters to keep the number of features constant\n",
    "3. train test split\n",
    "4. train  model\n",
    "5. test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first we will define a function that will use Gabor filters to reduce the images to a constant set of features\n",
    "#define Gabor features\n",
    "def compute_feats(image, kernels):\n",
    "    feats = np.zeros((len(kernels), 2), dtype=np.double)\n",
    "    for k, kernel in enumerate(kernels):\n",
    "        filtered = ndi.convolve(image, kernel, mode='wrap')\n",
    "        #feats[k, 0] = filtered.mean()\n",
    "        #feats[k, 1] = filtered.var()\n",
    "        feats[k, 0] = kurtosis(np.reshape(filtered,-1))\n",
    "        feats[k, 1] = skew(np.reshape(filtered,-1))\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta = 3.141592653589793, sigma = 1 frequency = 0.05\n",
      "theta = 3.141592653589793, sigma = 1 frequency = 0.25\n",
      "theta = 3.141592653589793, sigma = 4 frequency = 0.05\n",
      "theta = 3.141592653589793, sigma = 4 frequency = 0.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare Gabor filter bank kernels\n",
    "kernels = []\n",
    "for sigma in (1,4):\n",
    "    theta = np.pi\n",
    "    for frequency in (0.05, 0.25):\n",
    "        print('theta = {}, sigma = {} frequency = {}'.format(theta, sigma, frequency) )\n",
    "        kernel = np.real(gabor_kernel(frequency,theta=theta,sigma_x=sigma, sigma_y=sigma))\n",
    "        kernels.append(kernel)\n",
    "                         \n",
    "np.shape(kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load Santa images into an array\n",
    "santafolder = './Santa_Claus/'\n",
    "santa_images = glob.glob('{}*.jpg'.format(santafolder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load Santa images into an array\n",
    "nosantafolder = './nosanta/'\n",
    "nosanta_images = glob.glob('{}*.jpg'.format(nosantafolder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before staring the ML part, there is some preprocessing that needs to be done. The main issue with this dataset is that all the images are of random sizes. To use this as a train/test dataset, we can do two things:\n",
    "\n",
    "1. Use Convolutional Neural Networks \n",
    "2. Use an image feature reduction technique.\n",
    "\n",
    "We will use 2 in this tutorial (as I do not know how to implement a CNN!). The image reduction technique we will use is Gabor Filters to reduce the images to 8 features. See this post for details: http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_gabor.html\n",
    "\n",
    "**Note this will take some time!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "santa_feats = np.zeros((len(santa_images),9))\n",
    "for i, image in enumerate(santa_images):\n",
    "    im = plt.imread(image,format='jpeg')\n",
    "    if len(im.shape) > 2:\n",
    "        imean = im.mean(axis=2)\n",
    "    else:\n",
    "        imean = im\n",
    "    imfeats = compute_feats(imean,kernels).reshape(-1)\n",
    "    santa_feats[i,:-1] = imfeats \n",
    "    santa_feats[i,-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nosanta_feats = np.zeros((len(nosanta_images),9))\n",
    "for i, image in enumerate(nosanta_images):\n",
    "    im = plt.imread(image,format='jpeg')\n",
    "    imfeats = compute_feats(im.mean(axis=2),kernels).reshape(-1)\n",
    "    nosanta_feats[i,:-1] = imfeats \n",
    "    nosanta_feats[i,-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the datasets\n",
    "ds = np.concatenate((nosanta_feats,santa_feats), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ds[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "features = MaxAbsScaler().fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = ds[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data and target sizes: \n",
      "(491, 8), (491,)\n",
      "Test data and target sizes: \n",
      "(164, 8), (164,)\n"
     ]
    }
   ],
   "source": [
    "print('Training data and target sizes: \\n{}, {}'.format(X_train.shape,y_train.shape))\n",
    "print('Test data and target sizes: \\n{}, {}'.format(X_test.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a classifier: a support vector classifier\n",
    "classifier = svm.SVC(C=1,kernel='rbf',gamma=1)\n",
    "#fit to the training data\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now to Now predict the value of the digit on the test data\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[86  5]\n",
      " [49 24]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.64      0.95      0.76        91\n",
      "        1.0       0.83      0.33      0.47        73\n",
      "\n",
      "avg / total       0.72      0.67      0.63       164\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
